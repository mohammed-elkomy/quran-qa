# TCE at Quran QA 2022 & 2023
This repo contains the code submissions for both subtasks of the Quran QA competitions held in 2022 and 2023.

I led the Tanta Computer Engineering (TCE) team for two consecutive years and through our collective efforts,
we successfully achieved first place in both editions of the competition ðŸ¥‡ðŸ¥‡ðŸ¥‡.

All models can be found on my [huggingface account](https://huggingface.co/MatMulMan)
## Useful links
- [Submission Paper link for Quran QA 2023](https://aclanthology.org/2023.arabicnlp-1.81/)
- [Submission Paper link for Quran QA 2022](https://arxiv.org/abs/2206.01550)
- [Competition Link for Quran QA 2023](https://sites.google.com/view/quran-qa-2023/home?authuser=0)
- [Competition Link for Quran QA 2022](https://sites.google.com/view/quran-qa-2022)


## Citation

If you build upon or reference our work, please cite the following papers:

**1. Qur'an QA 2023**  
```bibtex
@inproceedings{TCE2023,
    title = "{TCE} at {Q}ur{'}an {QA} 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur{'}anic {QA}",
    author = "Mohamemd Elkomy and Sarhan, Amany",
    booktitle = "Proceedings of ArabicNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.arabicnlp-1.81",
    pages = "728--742",
    abstract = "In this paper, we present our approach to tackle Qur{'}an QA 2023 shared tasks A and B. To address the challenge of low-resourced training data, we rely on transfer learning together with a voting ensemble to improve prediction stability across multiple runs. Additionally, we employ different architectures and learning mechanisms for a range of Arabic pre-trained transformer-based models for both tasks. To identify unanswerable questions, we propose using a thresholding mechanism. Our top-performing systems greatly surpass the baseline performance on the hidden split, achieving a MAP score of 25.05{\%} for task A and a partial Average Precision (pAP) of 57.11{\%} for task B.",
}
```

**2. Qur'an QA 2022**  
```bibtex
@inproceedings{TCE,
    author = "Mohamemd Elkomy and Amany M Sarhan",
    title = "{TCE} at {Q}ur'an {QA} 2022: Arabic Language Question Answering Over Holy Qur'an Using a Post-Processed Ensemble of BERT-based Models",
    booktitle = "Proceedings of the 5th Workshop on Open-Source Arabic Corpora and Processing Tools with Shared Tasks on Qur'an QA and Fine-Grained Hate Speech Detection",
    month = "June",
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.osact-1.19",
    pages = "154-161",
}
```

## Contributions

Contributions to this repository are welcome! If you find any issues or have ideas for improvements, please submit an issue or a pull request. Please remember to give credit to this repository and cite the papers when using or referencing this work.

