{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDxbUn4SqkZn",
        "outputId": "4e0fa6ef-59dc-440c-9ca4-d7042f547a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr 14 07:16:54 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2k2ZNV3dVPz",
        "outputId": "eb9c514d-e845-4b11-b1b1-04f3d178bf2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mrc-qrcd'...\n",
            "remote: Enumerating objects: 317, done.\u001b[K\n",
            "remote: Counting objects: 100% (317/317), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 317 (delta 127), reused 317 (delta 127), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (317/317), 45.96 MiB | 27.16 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ]
        }
      ],
      "source": [
        "repo_url = f\"https://mohammed-elkomy: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@github.com/Masters-IR-ELKomy/mrc-qrcd.git\"\n",
        "!git clone $repo_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqlqPLD7dXvQ",
        "outputId": "d22a95c4-7954-492a-aea9-f9a39eecd405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:             12           0           9           0           2          11\n",
            "Swap:             0           0           0\n",
            "/content/mrc-qrcd\n"
          ]
        }
      ],
      "source": [
        "!free -g\n",
        "%cd mrc-qrcd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxZmUXEZRGs2",
        "outputId": "bcb83813-ae6b-4ac9-9741-3afca76e3802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 60.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 64.0 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 72.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 73.1 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 70.3 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 72.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n",
            "Collecting farasapy==0.0.14\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farasapy==0.0.14) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farasapy==0.0.14) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy==0.0.14) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy==0.0.14) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy==0.0.14) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy==0.0.14) (3.0.4)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 20.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.25.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=bff349d886874d8abced98635f75c5c76c35cb50ef09584305936aa95974392c\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.0 sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install farasapy==0.0.14\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzG9fqz6k7t3",
        "outputId": "9d832528-974f-4e74-940b-d38f69a7afa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRhiMJqqUfC-"
      },
      "source": [
        "# qrcd\n",
        "\n",
        "to reproduce results you need to set seeds \n",
        "for example --seed 8045 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw-nfEryi82n"
      },
      "source": [
        "###  Arabert Large"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval phase\n",
        "training on qrcd/qrcd_v1.1_train.jsonl only\n",
        "\n",
        "seed values to use here\n",
        "```8045, 32558, 79727 ,30429 ,48910 ,46840 ,24384 ,55067 ,13718 ,16213 ,63304 ,40732 ,38609 ,22228 ,71549``` \n",
        "for exampel for 8045 \n",
        "you are supposed to download those files from colab into your local machine\n",
        "1. ```bert-large-arabertv02_1-eval-8045.dump```\n",
        "2. ```bert-large-arabertv02_1-8045.json```\n"
      ],
      "metadata": {
        "id": "EZSMXUcFIqaU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4P_wdQHLj1ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0dc69b-f7ad-4523-b025-c9eb848410b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "log_level:20\n",
            "04/14/2022 08:32:43 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "04/14/2022 08:32:43 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=bert-large-arabertv02_1/runs/Apr14_08-32-43_8d20cc1b724d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=pRR,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=65.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=bert-large-arabertv02_1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=bert-large-arabertv02_1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=2,\n",
            "seed=8045,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/14/2022 08:32:44 - WARNING - datasets.builder - Using custom data configuration default-cd7047838bfb5afc\n",
            "04/14/2022 08:32:44 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/14/2022 08:32:44 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "04/14/2022 08:32:44 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba)\n",
            "04/14/2022 08:32:44 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "100% 2/2 [00:00<00:00, 292.79it/s]\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:32:44,669 >> loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ad052b31d73299ab90d34ca8cdfdbc073fc84ba4a424b31f456e31159b17431e.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:32:44,669 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "my configs BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.15,\n",
            "  \"classifier_dropout\": 0.3,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.15,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:32:45,225 >> loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ad052b31d73299ab90d34ca8cdfdbc073fc84ba4a424b31f456e31159b17431e.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:32:45,226 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:32:46,888 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/60e9ff28b93753c0d84eccffc9145d55e48e8993244148a01b72b99bb56acc31.292c9b563974181697c28c4ae4b6899dcaa7bdcf146b5682a389ef18208389a9\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:32:46,888 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/74b03364456a6ff9dc467962b740fe40d67a698eaf0ed8c0881fd836b1dec417.cf8d1bcc109a4ae89b00382b3c7dce421b2534c25751be3d5156322fc427fc12\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:32:46,888 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:32:46,889 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/13d0aeb6e3cef00c6474b9ff5fe5a23339fab57ac6cdf1640f741db78d01205a.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:32:46,889 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/a79ba04e38002933445acfd6e54a678537fe0d47b4c7ab8cb84e588b6adfb54d.b12f432f0d7c968f828692e21953045e83b24740d6985763c1f266215d87939a\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:32:47,165 >> loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ad052b31d73299ab90d34ca8cdfdbc073fc84ba4a424b31f456e31159b17431e.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:32:47,166 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1772] 2022-04-14 08:32:47,552 >> loading weights file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/5b6b65a52fdf663576d0fb750fd5af9041fbed101169d5b426f82f77cd80ab76.239c28acfb49bdb306b7935493251dfd48e2676d65a60030d1ad1b86bc2868e5\n",
            "[WARNING|modeling_utils.py:2049] 2022-04-14 08:32:57,736 >> Some weights of the model checkpoint at aubmindlab/bert-large-arabertv02 were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2060] 2022-04-14 08:32:57,736 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/bert-large-arabertv02 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:32:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-2c97d38ed3522c99.arrow\n",
            "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  1.45ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:32:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-3e7b9631af3602de.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  1.92ba/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[2022-04-14 08:32:59,176 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:453] 2022-04-14 08:33:09,009 >> Using amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1290] 2022-04-14 08:33:09,026 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2022-04-14 08:33:09,026 >>   Num examples = 712\n",
            "[INFO|trainer.py:1292] 2022-04-14 08:33:09,026 >>   Num Epochs = 65\n",
            "[INFO|trainer.py:1293] 2022-04-14 08:33:09,026 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1294] 2022-04-14 08:33:09,026 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1295] 2022-04-14 08:33:09,026 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2022-04-14 08:33:09,026 >>   Total optimization steps = 5785\n",
            "  0% 3/5785 [00:01<1:03:32,  1.52it/s]Traceback (most recent call last):\n",
            "  File \"run_qa.py\", line 737, in <module>\n",
            "    main()\n",
            "  File \"run_qa.py\", line 686, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1422, in train\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2011, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2043, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1839, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1006, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 592, in forward\n",
            "    output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 477, in forward\n",
            "    past_key_value=self_attn_past_key_value,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 409, in forward\n",
            "    output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 334, in forward\n",
            "    attention_probs = self.dropout(attention_probs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 1169, in dropout\n",
            "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 14.76 GiB total capacity; 13.21 GiB already allocated; 69.75 MiB free; 13.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "  0% 3/5785 [00:02<1:15:28,  1.28it/s]\n"
          ]
        }
      ],
      "source": [
        "!git pull\n",
        "!rm -r \"bert-large-arabertv02_1\"\n",
        "!python run_qa.py \\\n",
        "  --model_name_or_path \"aubmindlab/bert-large-arabertv02\" \\\n",
        "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 65 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --max_answer_length 35 \\\n",
        "  --output_dir \"bert-large-arabertv02_1\" \\\n",
        "  --overwrite_output_dir  \\\n",
        "  --overwrite_cache \\\n",
        "  --train_file qrcd/qrcd_v1.1_train.jsonl \\\n",
        "  --validation_file qrcd/qrcd_v1.1_dev.jsonl \\\n",
        "  --save_total_limit 2 \\\n",
        "  --save_strategy \"epoch\" \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --load_best_model_at_end  True \\\n",
        "  --metric_for_best_model 'pRR' \\\n",
        "  --greater_is_better True \\\n",
        "  --eval_metric \"./data/qrcd_metric.py\" --fp16 \\\n",
        "  --seed 8045 \n",
        "\n",
        "  # per_device_train_batch_size is 6 for T4\n",
        "  # per_device_train_batch_size is 8 for P100"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test phase\n",
        " training on qrcd_v1.1_train_dev.json (both train + eval)\n",
        "\n",
        " seed values to use here\n",
        "```1114 ,18695 ,23293 ,27892 ,5748 ,59131 ,63847 ,68498 ,73133 ,77793 ,82431 ,87062 ,91701 ,94452 ,96475 ,98797``` \n",
        "for exampel for 8045 \n",
        "you are supposed to download those files from colab into your local machine\n",
        "1. ```bert-large-arabertv02_1-predict-1114.dump```\n",
        "2. ```bert-large-arabertv02_1-1114.json```"
      ],
      "metadata": {
        "id": "qkkkBmrZI6hm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zWQOYO9hnK26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8562287-e516-41a1-fc10-6628cd826824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "log_level:20\n",
            "04/14/2022 08:34:02 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "04/14/2022 08:34:02 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=bert-large-arabertv02_1/runs/Apr14_08-34-02_8d20cc1b724d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=40.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=bert-large-arabertv02_1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=bert-large-arabertv02_1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=None,\n",
            "seed=1009,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/14/2022 08:34:02 - WARNING - datasets.builder - Using custom data configuration default-b2ae1bea58628d69\n",
            "04/14/2022 08:34:02 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/14/2022 08:34:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "04/14/2022 08:34:02 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba)\n",
            "04/14/2022 08:34:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "100% 2/2 [00:00<00:00, 542.71it/s]\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:34:02,789 >> loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ad052b31d73299ab90d34ca8cdfdbc073fc84ba4a424b31f456e31159b17431e.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:34:02,790 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "my configs BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.15,\n",
            "  \"classifier_dropout\": 0.3,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.15,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:34:03,342 >> loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ad052b31d73299ab90d34ca8cdfdbc073fc84ba4a424b31f456e31159b17431e.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:34:03,342 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:34:04,992 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/60e9ff28b93753c0d84eccffc9145d55e48e8993244148a01b72b99bb56acc31.292c9b563974181697c28c4ae4b6899dcaa7bdcf146b5682a389ef18208389a9\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:34:04,992 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/74b03364456a6ff9dc467962b740fe40d67a698eaf0ed8c0881fd836b1dec417.cf8d1bcc109a4ae89b00382b3c7dce421b2534c25751be3d5156322fc427fc12\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:34:04,992 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:34:04,992 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/13d0aeb6e3cef00c6474b9ff5fe5a23339fab57ac6cdf1640f741db78d01205a.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:34:04,992 >> loading file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/a79ba04e38002933445acfd6e54a678537fe0d47b4c7ab8cb84e588b6adfb54d.b12f432f0d7c968f828692e21953045e83b24740d6985763c1f266215d87939a\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:34:05,266 >> loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ad052b31d73299ab90d34ca8cdfdbc073fc84ba4a424b31f456e31159b17431e.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:34:05,267 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1772] 2022-04-14 08:34:05,622 >> loading weights file https://huggingface.co/aubmindlab/bert-large-arabertv02/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/5b6b65a52fdf663576d0fb750fd5af9041fbed101169d5b426f82f77cd80ab76.239c28acfb49bdb306b7935493251dfd48e2676d65a60030d1ad1b86bc2868e5\n",
            "[WARNING|modeling_utils.py:2049] 2022-04-14 08:34:10,659 >> Some weights of the model checkpoint at aubmindlab/bert-large-arabertv02 were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2060] 2022-04-14 08:34:10,659 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/bert-large-arabertv02 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:34:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-e67ab3132c3b79b6.arrow\n",
            "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  1.13ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:34:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-59d4199539915e41.arrow\n",
            "Running tokenizer on prediction dataset: 100% 1/1 [00:01<00:00,  1.38s/ba]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[2022-04-14 08:34:13,150 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:453] 2022-04-14 08:34:22,576 >> Using amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1290] 2022-04-14 08:34:22,591 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2022-04-14 08:34:22,591 >>   Num examples = 821\n",
            "[INFO|trainer.py:1292] 2022-04-14 08:34:22,591 >>   Num Epochs = 40\n",
            "[INFO|trainer.py:1293] 2022-04-14 08:34:22,591 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1294] 2022-04-14 08:34:22,591 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1295] 2022-04-14 08:34:22,591 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2022-04-14 08:34:22,591 >>   Total optimization steps = 4120\n",
            "  0% 6/4120 [00:03<43:33,  1.57it/s]Traceback (most recent call last):\n",
            "  File \"run_qa.py\", line 737, in <module>\n",
            "    main()\n",
            "  File \"run_qa.py\", line 686, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1422, in train\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2011, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2043, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1839, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1006, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 592, in forward\n",
            "    output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 477, in forward\n",
            "    past_key_value=self_attn_past_key_value,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 409, in forward\n",
            "    output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 334, in forward\n",
            "    attention_probs = self.dropout(attention_probs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 1169, in dropout\n",
            "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 14.76 GiB total capacity; 13.21 GiB already allocated; 69.75 MiB free; 13.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "  0% 6/4120 [00:04<46:55,  1.46it/s]\n"
          ]
        }
      ],
      "source": [
        "!git pull\n",
        "!rm -r \"bert-large-arabertv02_1\"\n",
        "!python run_qa.py \\\n",
        "  --model_name_or_path \"aubmindlab/bert-large-arabertv02\" \\\n",
        "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
        "  --do_train \\\n",
        "  --do_predict \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 40 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --max_answer_length 35 \\\n",
        "  --output_dir \"bert-large-arabertv02_1\" \\\n",
        "  --overwrite_output_dir  \\\n",
        "  --overwrite_cache \\\n",
        "  --train_file qrcd/qrcd_v1.1_train_dev.jsonl \\\n",
        "  --test_file qrcd/qrcd_v1.1_test_noAnswers.jsonl \\\n",
        "  --save_strategy \"no\" \\\n",
        "  --eval_metric \"./data/qrcd_metric.py\" --fp16 \\\n",
        "  --seed 1009  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okMfegc-jR5-"
      },
      "source": [
        "### Arabert-base"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval phase\n",
        "training on qrcd/qrcd_v1.1_train.jsonl only\n",
        "\n",
        "seed values to use here\n",
        "```71338 ,67981 ,29808 ,67961 ,25668 ,20181 ,20178 ,67985 ,67982 ,23415 ,20172 ,20166 ,25982 ,27073 ,26612```\n",
        "for exampel for 71338 \n",
        "you are supposed to download those files from colab into your local machine\n",
        "1. ```bert-base-arabertv02-eval-71338.dump```\n",
        "2. ```bert-base-arabertv02-71338.json```"
      ],
      "metadata": {
        "id": "OE_WSldwLJHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull\n",
        "!rm -r \"bert-base-arabertv02\"\n",
        "!python run_qa.py \\\n",
        "  --model_name_or_path  \"aubmindlab/bert-base-arabertv02\" \\\n",
        "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 50 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --max_answer_length 35 \\\n",
        "  --output_dir \"bert-base-arabertv02\" \\\n",
        "  --overwrite_output_dir  \\\n",
        "  --overwrite_cache \\\n",
        "  --train_file qrcd/qrcd_v1.1_train.jsonl \\\n",
        "  --validation_file qrcd/qrcd_v1.1_dev.jsonl \\\n",
        "  --save_total_limit 2 \\\n",
        "  --save_strategy \"epoch\" \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --load_best_model_at_end  True \\\n",
        "  --metric_for_best_model 'pRR' \\\n",
        "  --greater_is_better True \\\n",
        "  --eval_metric \"./data/qrcd_metric.py\" \\\n",
        "  --seed 71338"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4vqTmjue5uC",
        "outputId": "37938faf-6234-43d1-af57-4b19a8d7ef11"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "rm: cannot remove 'bert-base-arabertv02': No such file or directory\n",
            "log_level:20\n",
            "04/14/2022 08:35:00 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/14/2022 08:35:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=bert-base-arabertv02/runs/Apr14_08-35-00_8d20cc1b724d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=pRR,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=50.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=bert-base-arabertv02,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=bert-base-arabertv02,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=2,\n",
            "seed=71338,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/14/2022 08:35:00 - WARNING - datasets.builder - Using custom data configuration default-cd7047838bfb5afc\n",
            "04/14/2022 08:35:00 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/14/2022 08:35:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "04/14/2022 08:35:00 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba)\n",
            "04/14/2022 08:35:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "100% 2/2 [00:00<00:00, 680.78it/s]\n",
            "[INFO|hub.py:583] 2022-04-14 08:35:00,930 >> https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpko3lt0t2\n",
            "Downloading: 100% 384/384 [00:00<00:00, 450kB/s]\n",
            "[INFO|hub.py:587] 2022-04-14 08:35:01,207 >> storing https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/411eec8d9e12bf4c11eebebb4c5fecd46da787616f45bcfd6cb187e0917afae0.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
            "[INFO|hub.py:595] 2022-04-14 08:35:01,207 >> creating metadata file for /root/.cache/huggingface/transformers/411eec8d9e12bf4c11eebebb4c5fecd46da787616f45bcfd6cb187e0917afae0.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:35:01,207 >> loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/411eec8d9e12bf4c11eebebb4c5fecd46da787616f45bcfd6cb187e0917afae0.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:35:01,208 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "my configs BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.15,\n",
            "  \"classifier_dropout\": 0.3,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.15,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-04-14 08:35:01,481 >> https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfxbsf039\n",
            "Downloading: 100% 381/381 [00:00<00:00, 382kB/s]\n",
            "[INFO|hub.py:587] 2022-04-14 08:35:01,756 >> storing https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f8b57973211cf183180900e4646e204fe0c492b6129df1509f0e8ddc02369a05.130b8595f3c840efdfe3e2e9d3926b3c47a1a69aa1a7f6660fd11be56bf0d5fc\n",
            "[INFO|hub.py:595] 2022-04-14 08:35:01,757 >> creating metadata file for /root/.cache/huggingface/transformers/f8b57973211cf183180900e4646e204fe0c492b6129df1509f0e8ddc02369a05.130b8595f3c840efdfe3e2e9d3926b3c47a1a69aa1a7f6660fd11be56bf0d5fc\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:35:02,028 >> loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/411eec8d9e12bf4c11eebebb4c5fecd46da787616f45bcfd6cb187e0917afae0.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:35:02,029 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-04-14 08:35:02,574 >> https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpat4o9xab\n",
            "Downloading: 100% 805k/805k [00:00<00:00, 2.65MB/s]\n",
            "[INFO|hub.py:587] 2022-04-14 08:35:03,240 >> storing https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6d195dfa0ab2ff9e30278a35699eb13eeb3a69731e8458ef8af7c9598e05ee99.292c9b563974181697c28c4ae4b6899dcaa7bdcf146b5682a389ef18208389a9\n",
            "[INFO|hub.py:595] 2022-04-14 08:35:03,241 >> creating metadata file for /root/.cache/huggingface/transformers/6d195dfa0ab2ff9e30278a35699eb13eeb3a69731e8458ef8af7c9598e05ee99.292c9b563974181697c28c4ae4b6899dcaa7bdcf146b5682a389ef18208389a9\n",
            "[INFO|hub.py:583] 2022-04-14 08:35:03,519 >> https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphor4edl2\n",
            "Downloading: 100% 2.52M/2.52M [00:00<00:00, 6.84MB/s]\n",
            "[INFO|hub.py:587] 2022-04-14 08:35:04,301 >> storing https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/cdf2434ef735735269b56feeae539fb93d28d674f4b335e2f08ed38960ea51e7.cf8d1bcc109a4ae89b00382b3c7dce421b2534c25751be3d5156322fc427fc12\n",
            "[INFO|hub.py:595] 2022-04-14 08:35:04,301 >> creating metadata file for /root/.cache/huggingface/transformers/cdf2434ef735735269b56feeae539fb93d28d674f4b335e2f08ed38960ea51e7.cf8d1bcc109a4ae89b00382b3c7dce421b2534c25751be3d5156322fc427fc12\n",
            "[INFO|hub.py:583] 2022-04-14 08:35:04,850 >> https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpizt4vkg4\n",
            "Downloading: 100% 112/112 [00:00<00:00, 132kB/s]\n",
            "[INFO|hub.py:587] 2022-04-14 08:35:05,128 >> storing https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/4318ddb7acd24ce55328cd47235c5aaf88d4925a8a257863c4ebcc0852985d2c.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|hub.py:595] 2022-04-14 08:35:05,128 >> creating metadata file for /root/.cache/huggingface/transformers/4318ddb7acd24ce55328cd47235c5aaf88d4925a8a257863c4ebcc0852985d2c.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:05,409 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6d195dfa0ab2ff9e30278a35699eb13eeb3a69731e8458ef8af7c9598e05ee99.292c9b563974181697c28c4ae4b6899dcaa7bdcf146b5682a389ef18208389a9\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:05,409 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/cdf2434ef735735269b56feeae539fb93d28d674f4b335e2f08ed38960ea51e7.cf8d1bcc109a4ae89b00382b3c7dce421b2534c25751be3d5156322fc427fc12\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:05,409 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:05,409 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/4318ddb7acd24ce55328cd47235c5aaf88d4925a8a257863c4ebcc0852985d2c.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:05,409 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f8b57973211cf183180900e4646e204fe0c492b6129df1509f0e8ddc02369a05.130b8595f3c840efdfe3e2e9d3926b3c47a1a69aa1a7f6660fd11be56bf0d5fc\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:35:05,694 >> loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/411eec8d9e12bf4c11eebebb4c5fecd46da787616f45bcfd6cb187e0917afae0.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:35:05,694 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-04-14 08:35:06,054 >> https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzifusacf\n",
            "Downloading: 100% 518M/518M [00:08<00:00, 64.8MB/s]\n",
            "[INFO|hub.py:587] 2022-04-14 08:35:14,515 >> storing https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/c0183275aaa055648fb44d6c24d9604b860b02398c7d9c7bea64257cdb87a56e.9e74f71b2acf9ee9289bd6ab56938934df2792f595150523e3f83558666a9676\n",
            "[INFO|hub.py:595] 2022-04-14 08:35:14,515 >> creating metadata file for /root/.cache/huggingface/transformers/c0183275aaa055648fb44d6c24d9604b860b02398c7d9c7bea64257cdb87a56e.9e74f71b2acf9ee9289bd6ab56938934df2792f595150523e3f83558666a9676\n",
            "[INFO|modeling_utils.py:1772] 2022-04-14 08:35:14,516 >> loading weights file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/c0183275aaa055648fb44d6c24d9604b860b02398c7d9c7bea64257cdb87a56e.9e74f71b2acf9ee9289bd6ab56938934df2792f595150523e3f83558666a9676\n",
            "[WARNING|modeling_utils.py:2049] 2022-04-14 08:35:16,242 >> Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2060] 2022-04-14 08:35:16,243 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"run_qa.py\", line 737, in <module>\n",
            "  File \"run_qa.py\", line 519, in main\n",
            "    desc=\"Running tokenizer on train dataset\",\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\", line 1971, in map\n",
            "    desc=desc,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\", line 519, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\", line 486, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/fingerprint.py\", line 458, in wrapper\n",
            "    out = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\", line 2341, in _map_single\n",
            "    offset=offset,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\", line 2218, in apply_function_on_filtered_inputs\n",
            "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\", line 1913, in decorated\n",
            "    result = f(decorated_item, *args, **kwargs)\n",
            "  File \"run_qa.py\", line 426, in prepare_train_features\n",
            "    while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test phase\n",
        " training on qrcd_v1.1_train_dev.json (both train + eval)\n",
        "\n",
        " seed values to use here\n",
        "```54235 ,60998 ,64662 ,80936 ,80955 ,80959 ,80970 ,80988 ,82916 ,84448 ,84481 ,84665 ,84749 ,84871 ,87891 ,87917 ,88329 ,88469```\n",
        "for exampel for 54235 \n",
        "you are supposed to download those files from colab into your local machine\n",
        "1. ```bert-large-arabertv02_1-predict-54235.dump```\n",
        "2. ```bert-base-arabertv02-54235.json```"
      ],
      "metadata": {
        "id": "eJYXSBXeLK_x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzurAveHjTcW",
        "outputId": "87200529-343d-49b2-8ad5-ca49b3cfde22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "log_level:20\n",
            "04/14/2022 08:35:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/14/2022 08:35:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=bert-base-arabertv02/runs/Apr14_08-35-36_8d20cc1b724d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=32.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=bert-base-arabertv02,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=bert-base-arabertv02,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=None,\n",
            "seed=54235,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/14/2022 08:35:36 - WARNING - datasets.builder - Using custom data configuration default-b2ae1bea58628d69\n",
            "04/14/2022 08:35:36 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/14/2022 08:35:36 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "04/14/2022 08:35:36 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba)\n",
            "04/14/2022 08:35:36 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "100% 2/2 [00:00<00:00, 683.72it/s]\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:35:37,256 >> loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/411eec8d9e12bf4c11eebebb4c5fecd46da787616f45bcfd6cb187e0917afae0.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:35:37,257 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "my configs BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.15,\n",
            "  \"classifier_dropout\": 0.3,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.15,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:35:37,802 >> loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/411eec8d9e12bf4c11eebebb4c5fecd46da787616f45bcfd6cb187e0917afae0.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:35:37,803 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:39,445 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6d195dfa0ab2ff9e30278a35699eb13eeb3a69731e8458ef8af7c9598e05ee99.292c9b563974181697c28c4ae4b6899dcaa7bdcf146b5682a389ef18208389a9\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:39,445 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/cdf2434ef735735269b56feeae539fb93d28d674f4b335e2f08ed38960ea51e7.cf8d1bcc109a4ae89b00382b3c7dce421b2534c25751be3d5156322fc427fc12\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:39,446 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:39,446 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/4318ddb7acd24ce55328cd47235c5aaf88d4925a8a257863c4ebcc0852985d2c.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:39,446 >> loading file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f8b57973211cf183180900e4646e204fe0c492b6129df1509f0e8ddc02369a05.130b8595f3c840efdfe3e2e9d3926b3c47a1a69aa1a7f6660fd11be56bf0d5fc\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:35:39,717 >> loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/411eec8d9e12bf4c11eebebb4c5fecd46da787616f45bcfd6cb187e0917afae0.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:35:39,718 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1772] 2022-04-14 08:35:40,080 >> loading weights file https://huggingface.co/aubmindlab/bert-base-arabertv02/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/c0183275aaa055648fb44d6c24d9604b860b02398c7d9c7bea64257cdb87a56e.9e74f71b2acf9ee9289bd6ab56938934df2792f595150523e3f83558666a9676\n",
            "[WARNING|modeling_utils.py:2049] 2022-04-14 08:35:41,821 >> Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2060] 2022-04-14 08:35:41,821 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:35:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-1ca3c1843a3c0793.arrow\n",
            "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  2.53ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:35:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-c09dd0daf5d91111.arrow\n",
            "Running tokenizer on prediction dataset: 100% 1/1 [00:01<00:00,  1.19s/ba]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[2022-04-14 08:35:43,643 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Traceback (most recent call last):\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git pull\n",
        "!rm -r \"bert-base-arabertv02\"\n",
        "!python run_qa.py \\\n",
        "  --model_name_or_path  \"aubmindlab/bert-base-arabertv02\" \\\n",
        "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
        "  --do_train \\\n",
        "  --do_predict \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 32 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --max_answer_length 35 \\\n",
        "  --output_dir \"bert-base-arabertv02\" \\\n",
        "  --overwrite_output_dir  \\\n",
        "  --overwrite_cache \\\n",
        "  --train_file qrcd/qrcd_v1.1_train_dev.jsonl \\\n",
        "  --test_file qrcd/qrcd_v1.1_test_noAnswers.jsonl \\\n",
        "  --save_strategy \"no\" \\\n",
        "  --eval_metric \"./data/qrcd_metric.py\" \\\n",
        "  --seed 54235"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxJoExNti-Pe"
      },
      "source": [
        "### ARBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval phase\n",
        "training on qrcd/qrcd_v1.1_train.jsonl only\n",
        "\n",
        "seed values to use here\n",
        "```64976 ,64988 ,73862 ,84804 ,79583 ,81181 ,59377 ,59382 ,73869 ,77564 ,79723 ,64952 ,73865 ,59373 ,84349``` \n",
        "for exampel for 64976 \n",
        "you are supposed to download those files from colab into your local machine\n",
        "1. ```ARBERT-eval-64976.dump```\n",
        "2. ```ARBERT-64976.json```\n"
      ],
      "metadata": {
        "id": "Rd39qyCuLQ-A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp5B-bcDjFPe",
        "outputId": "79a5db0f-a4cd-4053-ca3d-68585894ded9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "log_level:20\n",
            "04/14/2022 08:35:54 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/14/2022 08:35:54 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=ARBERT/runs/Apr14_08-35-54_8d20cc1b724d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=pRR,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=50.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=ARBERT,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=ARBERT,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=2,\n",
            "seed=64976,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/14/2022 08:35:54 - WARNING - datasets.builder - Using custom data configuration default-cd7047838bfb5afc\n",
            "04/14/2022 08:35:54 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/14/2022 08:35:54 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "04/14/2022 08:35:54 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba)\n",
            "04/14/2022 08:35:54 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "100% 2/2 [00:00<00:00, 637.92it/s]\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:35:55,132 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:35:55,132 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "my configs BertConfig {\n",
            "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.15,\n",
            "  \"classifier_dropout\": 0.3,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.15,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:35:55,701 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:35:55,701 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:57,352 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b666228ad7d67661ab245a67d10a34dfa93d9fd5de5ef3c6a607a92a9db04766.48871a2b1d848c60db4344a74b461797d9913433a790a90861ff76b4e847d1bc\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:57,352 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:57,352 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:57,352 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/110b3bd85e0675de74f72d1ba2e29b31cd09cd69aa8f70d9037b706f3bcc26fb.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:35:57,353 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/41d233d78ec59e1817ce96e1283631b3579e030eb979d766e25eaf3e1f0362ff.e2391f37093ecddb817287261d0566d2f027887d636f1764ee422c1977ae6ac1\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:35:57,630 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:35:57,631 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:35:58,027 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:35:58,028 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1772] 2022-04-14 08:35:58,377 >> loading weights file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f37a41178e5c357491b1adb9fe079d6e41ff54338328ea61fbd2a155b6c782ea.ef8eca7a93e9680c24baaba838665035caca0660ec8613de69adea7f19b57817\n",
            "[WARNING|modeling_utils.py:2049] 2022-04-14 08:36:02,603 >> Some weights of the model checkpoint at UBC-NLP/ARBERT were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2060] 2022-04-14 08:36:02,603 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at UBC-NLP/ARBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:36:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-4f1d2412b8589d81.arrow\n",
            "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  2.20ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:36:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-cd7047838bfb5afc/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-1873f793a57502c8.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  2.02ba/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[2022-04-14 08:36:03,757 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Traceback (most recent call last):\n",
            "  File \"run_qa.py\", line 737, in <module>\n",
            "    main()\n",
            "  File \"run_qa.py\", line 657, in main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 1395, in load_metric\n",
            "    metric_cls = import_main_class(metric_module, dataset=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 106, in import_main_class\n",
            "    module = importlib.import_module(module_path)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/metrics/qrcd_metric/c7f476a4a9147242a1c8b46c61cfd7505b59aa7be38faecbe820cf17f59c9dcf/qrcd_metric.py\", line 142, in <module>\n",
            "    from .qrcd_eval import evaluate\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/metrics/qrcd_metric/c7f476a4a9147242a1c8b46c61cfd7505b59aa7be38faecbe820cf17f59c9dcf/qrcd_eval.py\", line 18, in <module>\n",
            "    farasa_segmenter = FarasaSegmenter(interactive=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 54, in __init__\n",
            "    self._initialize_task()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 140, in _initialize_task\n",
            "    return self._run_task_interactive(bword)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 215, in _run_task_interactive\n",
            "    output = self.__task_proc.stdout.readline().decode(\"utf8\").strip()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!git pull\n",
        "!rm -r \"ARBERT\"\n",
        "!python run_qa.py \\\n",
        "  --model_name_or_path  \"UBC-NLP/ARBERT\" \\\n",
        "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 50 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --max_answer_length 35 \\\n",
        "  --output_dir \"ARBERT\" \\\n",
        "  --overwrite_output_dir  \\\n",
        "  --overwrite_cache \\\n",
        "  --train_file qrcd/qrcd_v1.1_train.jsonl \\\n",
        "  --validation_file qrcd/qrcd_v1.1_dev.jsonl \\\n",
        "  --save_total_limit 2 \\\n",
        "  --save_strategy \"epoch\" \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --load_best_model_at_end  True \\\n",
        "  --metric_for_best_model 'pRR' \\\n",
        "  --greater_is_better True \\\n",
        "  --eval_metric \"./data/qrcd_metric.py\" \\\n",
        "  --seed 64976"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test phase\n",
        " training on qrcd_v1.1_train_dev.json (both train + eval)\n",
        "\n",
        " seed values to use here\n",
        "```107 ,14 ,43919 ,47360 ,50798 ,57621 ,86829 ,88813 ,90781 ,91496 ,91533 ,94949 ,95000 ,96521 ,96552 ,98412 ,98465``` \n",
        "for exampel for 107 \n",
        "you are supposed to download those files from colab into your local machine\n",
        "1. ```ARBERT-predict-107.dump```\n",
        "2. ```ARBERT-107.json```"
      ],
      "metadata": {
        "id": "P0RJ0doSLPYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull\n",
        "!rm -r \"ARBERT\"\n",
        "!python run_qa.py \\\n",
        "  --model_name_or_path  \"UBC-NLP/ARBERT\" \\\n",
        "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
        "  --do_train \\\n",
        "  --do_predict \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 32 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --max_answer_length 35 \\\n",
        "  --output_dir \"ARBERT\" \\\n",
        "  --overwrite_output_dir  \\\n",
        "  --overwrite_cache \\\n",
        "  --train_file qrcd/qrcd_v1.1_train_dev.jsonl \\\n",
        "  --test_file qrcd/qrcd_v1.1_test_noAnswers.jsonl \\\n",
        "  --save_strategy \"no\" \\\n",
        "  --eval_metric \"./data/qrcd_metric.py\" \\\n",
        "  --seed 107"
      ],
      "metadata": {
        "id": "0idNmCIlOEoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c13b0a5-6cca-400d-d024-f61c2ce14fe8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "log_level:20\n",
            "04/14/2022 08:36:13 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/14/2022 08:36:13 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=ARBERT/runs/Apr14_08-36-13_8d20cc1b724d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=32.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=ARBERT,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=ARBERT,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=None,\n",
            "seed=107,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/14/2022 08:36:14 - WARNING - datasets.builder - Using custom data configuration default-b2ae1bea58628d69\n",
            "04/14/2022 08:36:14 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/14/2022 08:36:14 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "04/14/2022 08:36:14 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba)\n",
            "04/14/2022 08:36:14 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba\n",
            "100% 2/2 [00:00<00:00, 448.83it/s]\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:36:14,465 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:36:14,466 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "my configs BertConfig {\n",
            "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.15,\n",
            "  \"classifier_dropout\": 0.3,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.15,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:36:15,019 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:36:15,020 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:36:16,677 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b666228ad7d67661ab245a67d10a34dfa93d9fd5de5ef3c6a607a92a9db04766.48871a2b1d848c60db4344a74b461797d9913433a790a90861ff76b4e847d1bc\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:36:16,677 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:36:16,677 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:36:16,677 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/110b3bd85e0675de74f72d1ba2e29b31cd09cd69aa8f70d9037b706f3bcc26fb.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-14 08:36:16,677 >> loading file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/41d233d78ec59e1817ce96e1283631b3579e030eb979d766e25eaf3e1f0362ff.e2391f37093ecddb817287261d0566d2f027887d636f1764ee422c1977ae6ac1\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:36:16,949 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:36:16,950 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-04-14 08:36:17,339 >> loading configuration file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/434c80c85742eeafe4ddb060dbfb2ae47dbf7d55834acf27eb1a619e2b7bd311.897c52772d161fa08b3d031e0e548347e8ae1f81959cd6e58ccf096423ca5a11\n",
            "[INFO|configuration_utils.py:690] 2022-04-14 08:36:17,340 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"UBC-NLP/ARBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1772] 2022-04-14 08:36:17,707 >> loading weights file https://huggingface.co/UBC-NLP/ARBERT/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f37a41178e5c357491b1adb9fe079d6e41ff54338328ea61fbd2a155b6c782ea.ef8eca7a93e9680c24baaba838665035caca0660ec8613de69adea7f19b57817\n",
            "[WARNING|modeling_utils.py:2049] 2022-04-14 08:36:19,680 >> Some weights of the model checkpoint at UBC-NLP/ARBERT were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2060] 2022-04-14 08:36:19,681 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at UBC-NLP/ARBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:36:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-db7a282f2bb70a9d.arrow\n",
            "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  1.99ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]04/14/2022 08:36:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-b2ae1bea58628d69/0.0.0/79cdb942cf5ffe80e5e9539bde93fdeeada931092f07d56561dfe2d0ec0180ba/cache-6b874c14a49a95d2.arrow\n",
            "Running tokenizer on prediction dataset: 100% 1/1 [00:01<00:00,  1.21s/ba]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[2022-04-14 08:36:21,579 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Traceback (most recent call last):\n",
            "  File \"run_qa.py\", line 737, in <module>\n",
            "    main()\n",
            "  File \"run_qa.py\", line 657, in main\n",
            "    metric = load_metric(metric_file)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 1395, in load_metric\n",
            "    metric_cls = import_main_class(metric_module, dataset=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 106, in import_main_class\n",
            "    module = importlib.import_module(module_path)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/metrics/qrcd_metric/c7f476a4a9147242a1c8b46c61cfd7505b59aa7be38faecbe820cf17f59c9dcf/qrcd_metric.py\", line 142, in <module>\n",
            "    from .qrcd_eval import evaluate\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/metrics/qrcd_metric/c7f476a4a9147242a1c8b46c61cfd7505b59aa7be38faecbe820cf17f59c9dcf/qrcd_eval.py\", line 18, in <module>\n",
            "    farasa_segmenter = FarasaSegmenter(interactive=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 54, in __init__\n",
            "    self._initialize_task()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 140, in _initialize_task\n",
            "    return self._run_task_interactive(bword)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/farasa/__base.py\", line 215, in _run_task_interactive\n",
            "    output = self.__task_proc.stdout.readline().decode(\"utf8\").strip()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HN9JYTbjJ7l"
      },
      "source": [
        "### Other runs not included in the reported scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTO9ssIMjNqO",
        "outputId": "38553cb6-498e-4c9f-e986-ca408227c90d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n",
            "rm: cannot remove 'qarib1': No such file or directory\n",
            "log_level:20\n",
            "03/15/2022 12:02:16 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "03/15/2022 12:02:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=qarib1/runs/Mar15_12-02-16_341417b62003,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=pRR,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=40.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=qarib1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=qarib1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=5,\n",
            "seed=45736,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "03/15/2022 12:02:16 - WARNING - datasets.builder - Using custom data configuration default-12fd5740a033cd66\n",
            "03/15/2022 12:02:16 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2022 12:02:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
            "03/15/2022 12:02:16 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8)\n",
            "03/15/2022 12:02:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
            "100% 3/3 [00:00<00:00, 513.94it/s]\n",
            "[INFO|file_utils.py:2215] 2022-03-15 12:02:17,097 >> https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpx7pblms7\n",
            "Downloading: 100% 576/576 [00:00<00:00, 688kB/s]\n",
            "[INFO|file_utils.py:2219] 2022-03-15 12:02:17,235 >> storing https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|file_utils.py:2227] 2022-03-15 12:02:17,235 >> creating metadata file for /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:02:17,236 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:02:17,237 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "my configs BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.15,\n",
            "  \"classifier_dropout\": 0.3,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.15,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"run_qa.py\", line 712, in <module>\n",
            "    main()\n",
            "  File \"run_qa.py\", line 314, in main\n",
            "    use_auth_token=True if model_args.use_auth_token else None,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\", line 471, in from_pretrained\n",
            "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\", line 341, in get_tokenizer_config\n",
            "    local_files_only=local_files_only,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 2317, in get_file_from_repo\n",
            "    use_auth_token=use_auth_token,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 1929, in cached_path\n",
            "    local_files_only=local_files_only,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 2124, in get_from_cache\n",
            "    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/api.py\", line 104, in head\n",
            "    return request('head', url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/api.py\", line 61, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 530, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 643, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/adapters.py\", line 449, in send\n",
            "    timeout=timeout\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 677, in urlopen\n",
            "    chunked=chunked,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
            "    self._validate_conn(conn)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
            "    conn.connect()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\", line 309, in connect\n",
            "    conn = self._new_conn()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\", line 160, in _new_conn\n",
            "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/util/connection.py\", line 74, in create_connection\n",
            "    sock.connect(sa)\n",
            "KeyboardInterrupt\n",
            "Already up to date.\n",
            "rm: cannot remove 'qarib2': No such file or directory\n",
            "log_level:20\n",
            "03/15/2022 12:02:23 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "03/15/2022 12:02:23 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=qarib2/runs/Mar15_12-02-23_341417b62003,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=pRR,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=40.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=qarib2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=qarib2,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=5,\n",
            "seed=45743,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "03/15/2022 12:02:23 - WARNING - datasets.builder - Using custom data configuration default-12fd5740a033cd66\n",
            "03/15/2022 12:02:23 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2022 12:02:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
            "03/15/2022 12:02:23 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8)\n",
            "03/15/2022 12:02:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
            "100% 3/3 [00:00<00:00, 542.27it/s]\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:02:23,811 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:02:23,812 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "my configs BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.15,\n",
            "  \"classifier_dropout\": 0.3,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.15,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:344] 2022-03-15 12:02:23,945 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:02:24,084 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:02:24,084 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2215] 2022-03-15 12:02:24,366 >> https://huggingface.co/qarib/bert-base-qarib/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0q8j1rlk\n",
            "Downloading: 100% 701k/701k [00:00<00:00, 4.76MB/s]\n",
            "[INFO|file_utils.py:2219] 2022-03-15 12:02:24,656 >> storing https://huggingface.co/qarib/bert-base-qarib/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/4e43b9d2cde15e084228ed9ee0cd4bb9b9eb493ed463143ff05be6c7c76745e0.1e2b532a87919a27c2d3c3967b9e8e6e2b489997dda2b3d4a94173017625c844\n",
            "[INFO|file_utils.py:2227] 2022-03-15 12:02:24,656 >> creating metadata file for /root/.cache/huggingface/transformers/4e43b9d2cde15e084228ed9ee0cd4bb9b9eb493ed463143ff05be6c7c76745e0.1e2b532a87919a27c2d3c3967b9e8e6e2b489997dda2b3d4a94173017625c844\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:02:25,239 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4e43b9d2cde15e084228ed9ee0cd4bb9b9eb493ed463143ff05be6c7c76745e0.1e2b532a87919a27c2d3c3967b9e8e6e2b489997dda2b3d4a94173017625c844\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:02:25,240 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:02:25,240 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:02:25,240 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:02:25,240 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:02:25,380 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:02:25,381 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:02:25,592 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:02:25,593 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2215] 2022-03-15 12:02:25,804 >> https://huggingface.co/qarib/bert-base-qarib/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbhdda4d3\n",
            "Downloading: 100% 518M/518M [00:25<00:00, 21.1MB/s]\n",
            "[INFO|file_utils.py:2219] 2022-03-15 12:02:51,843 >> storing https://huggingface.co/qarib/bert-base-qarib/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/5d872f87d9f8e687e4af0afcfea853309cd273d61c78e5e9dadc8e7b14bb6d3e.2d3d7320a57d47d1644be78a8251e595e34f22616b29e0775ed12a6ce6ac6eab\n",
            "[INFO|file_utils.py:2227] 2022-03-15 12:02:51,843 >> creating metadata file for /root/.cache/huggingface/transformers/5d872f87d9f8e687e4af0afcfea853309cd273d61c78e5e9dadc8e7b14bb6d3e.2d3d7320a57d47d1644be78a8251e595e34f22616b29e0775ed12a6ce6ac6eab\n",
            "[INFO|modeling_utils.py:1431] 2022-03-15 12:02:51,844 >> loading weights file https://huggingface.co/qarib/bert-base-qarib/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/5d872f87d9f8e687e4af0afcfea853309cd273d61c78e5e9dadc8e7b14bb6d3e.2d3d7320a57d47d1644be78a8251e595e34f22616b29e0775ed12a6ce6ac6eab\n",
            "[WARNING|modeling_utils.py:1694] 2022-03-15 12:02:54,073 >> Some weights of the model checkpoint at qarib/bert-base-qarib were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1705] 2022-03-15 12:02:54,073 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at qarib/bert-base-qarib and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:02:54 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-cf6826cad1a41ea8.arrow\n",
            "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  1.82ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:02:54 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-8224c2ca68ab3036.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  1.67ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:02:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-57a98f5b05932d61.arrow\n",
            "Running tokenizer on prediction dataset: 100% 1/1 [00:00<00:00,  1.34ba/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[2022-03-15 12:02:56,208 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1279] 2022-03-15 12:03:03,987 >> ***** Running training *****\n",
            "[INFO|trainer.py:1280] 2022-03-15 12:03:03,987 >>   Num examples = 712\n",
            "[INFO|trainer.py:1281] 2022-03-15 12:03:03,987 >>   Num Epochs = 40\n",
            "[INFO|trainer.py:1282] 2022-03-15 12:03:03,987 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1283] 2022-03-15 12:03:03,987 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1284] 2022-03-15 12:03:03,988 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1285] 2022-03-15 12:03:03,988 >>   Total optimization steps = 1800\n",
            "{'loss': 1.806, 'learning_rate': 1.4444444444444446e-05, 'epoch': 11.11}\n",
            "{'loss': 0.4586, 'learning_rate': 8.888888888888888e-06, 'epoch': 22.22}\n",
            " 64% 1160/1800 [50:54<28:21,  2.66s/it]Traceback (most recent call last):\n",
            "  File \"run_qa.py\", line 712, in <module>\n",
            "    main()\n",
            "  File \"run_qa.py\", line 659, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1400, in train\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2002, in training_step\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 307, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 156, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n",
            " 64% 1160/1800 [50:56<28:06,  2.64s/it]\n",
            "Already up to date.\n",
            "rm: cannot remove 'qarib3': No such file or directory\n",
            "log_level:20\n",
            "03/15/2022 12:54:06 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "03/15/2022 12:54:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=qarib3/runs/Mar15_12-54-06_341417b62003,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=pRR,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=40.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=qarib3,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=qarib3,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=5,\n",
            "seed=48846,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "03/15/2022 12:54:06 - WARNING - datasets.builder - Using custom data configuration default-12fd5740a033cd66\n",
            "03/15/2022 12:54:06 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2022 12:54:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
            "03/15/2022 12:54:06 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8)\n",
            "03/15/2022 12:54:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
            "100% 3/3 [00:00<00:00, 663.45it/s]\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:54:07,134 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:54:07,135 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "my configs BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.15,\n",
            "  \"classifier_dropout\": 0.3,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.15,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:344] 2022-03-15 12:54:07,281 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:54:07,410 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:54:07,411 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:08,215 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4e43b9d2cde15e084228ed9ee0cd4bb9b9eb493ed463143ff05be6c7c76745e0.1e2b532a87919a27c2d3c3967b9e8e6e2b489997dda2b3d4a94173017625c844\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:08,215 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:08,215 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:08,216 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:08,216 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:54:08,352 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:54:08,353 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:54:08,554 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:54:08,554 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1431] 2022-03-15 12:54:08,751 >> loading weights file https://huggingface.co/qarib/bert-base-qarib/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/5d872f87d9f8e687e4af0afcfea853309cd273d61c78e5e9dadc8e7b14bb6d3e.2d3d7320a57d47d1644be78a8251e595e34f22616b29e0775ed12a6ce6ac6eab\n",
            "Traceback (most recent call last):\n",
            "  File \"run_qa.py\", line 712, in <module>\n",
            "    main()\n",
            "  File \"run_qa.py\", line 329, in main\n",
            "    use_auth_token=True if model_args.use_auth_token else None,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/auto_factory.py\", line 447, in from_pretrained\n",
            "    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\", line 1493, in from_pretrained\n",
            "    model = cls(config, *model_args, **model_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 1791, in __init__\n",
            "    self.bert = BertModel(config, add_pooling_layer=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 866, in __init__\n",
            "    self.encoder = BertEncoder(config)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 534, in __init__\n",
            "    self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 534, in <listcomp>\n",
            "    self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 450, in __init__\n",
            "    self.attention = BertAttention(config)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 370, in __init__\n",
            "    self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\", line 241, in __init__\n",
            "    self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 90, in __init__\n",
            "    self.reset_parameters()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 96, in reset_parameters\n",
            "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/init.py\", line 395, in kaiming_uniform_\n",
            "    return tensor.uniform_(-bound, bound)\n",
            "KeyboardInterrupt\n",
            "Already up to date.\n",
            "rm: cannot remove 'qarib4': No such file or directory\n",
            "log_level:20\n",
            "03/15/2022 12:54:16 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "03/15/2022 12:54:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=qarib4/runs/Mar15_12-54-16_341417b62003,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=pRR,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=40.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=qarib4,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=qarib4,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=5,\n",
            "seed=48856,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "03/15/2022 12:54:16 - WARNING - datasets.builder - Using custom data configuration default-12fd5740a033cd66\n",
            "03/15/2022 12:54:16 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "03/15/2022 12:54:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
            "03/15/2022 12:54:16 - WARNING - datasets.builder - Reusing dataset qrcd (/root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8)\n",
            "03/15/2022 12:54:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8\n",
            "100% 3/3 [00:00<00:00, 616.30it/s]\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:54:16,616 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:54:16,617 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "my configs BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.15,\n",
            "  \"classifier_dropout\": 0.3,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.15,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:344] 2022-03-15 12:54:16,785 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:54:16,923 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:54:16,923 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:17,754 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4e43b9d2cde15e084228ed9ee0cd4bb9b9eb493ed463143ff05be6c7c76745e0.1e2b532a87919a27c2d3c3967b9e8e6e2b489997dda2b3d4a94173017625c844\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:17,754 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:17,754 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:17,754 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-15 12:54:17,754 >> loading file https://huggingface.co/qarib/bert-base-qarib/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:54:17,895 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:54:17,896 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:648] 2022-03-15 12:54:18,107 >> loading configuration file https://huggingface.co/qarib/bert-base-qarib/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c0ec6f53275728b73899f82cdc4ea56aadacff5173093f57bf7b967222103917.368c7a0fea113d27d0f0362f0eef938f72f731bb3ba6858f0f30117a7f1bbe76\n",
            "[INFO|configuration_utils.py:684] 2022-03-15 12:54:18,108 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"qarib/bert-base-qarib\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1431] 2022-03-15 12:54:18,303 >> loading weights file https://huggingface.co/qarib/bert-base-qarib/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/5d872f87d9f8e687e4af0afcfea853309cd273d61c78e5e9dadc8e7b14bb6d3e.2d3d7320a57d47d1644be78a8251e595e34f22616b29e0775ed12a6ce6ac6eab\n",
            "[WARNING|modeling_utils.py:1694] 2022-03-15 12:54:20,454 >> Some weights of the model checkpoint at qarib/bert-base-qarib were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1705] 2022-03-15 12:54:20,454 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at qarib/bert-base-qarib and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:54:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-56caba4c5eb7d701.arrow\n",
            "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  1.94ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:54:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-8224c2ca68ab3036.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  1.81ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]03/15/2022 12:54:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/qrcd/default-12fd5740a033cd66/0.0.0/d54857cd763e7239dcb818fb3db872b5681d94105c90e87486762577abe4e5f8/cache-57a98f5b05932d61.arrow\n",
            "Running tokenizer on prediction dataset: 100% 1/1 [00:00<00:00,  1.37ba/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[2022-03-15 12:54:22,476 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1279] 2022-03-15 12:54:30,037 >> ***** Running training *****\n",
            "[INFO|trainer.py:1280] 2022-03-15 12:54:30,037 >>   Num examples = 712\n",
            "[INFO|trainer.py:1281] 2022-03-15 12:54:30,037 >>   Num Epochs = 40\n",
            "[INFO|trainer.py:1282] 2022-03-15 12:54:30,037 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1283] 2022-03-15 12:54:30,037 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1284] 2022-03-15 12:54:30,037 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1285] 2022-03-15 12:54:30,037 >>   Total optimization steps = 1800\n",
            "  0% 1/1800 [00:02<1:19:24,  2.65s/it]"
          ]
        }
      ],
      "source": [
        "!git pull\n",
        "!rm -r \"qarib\"\n",
        "!python run_qa.py \\\n",
        "  --model_name_or_path \"qarib/bert-base-qarib\" \\\n",
        "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 50 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --max_answer_length 35 \\\n",
        "  --output_dir \"qarib\" \\\n",
        "  --overwrite_output_dir  \\\n",
        "  --overwrite_cache \\\n",
        "  --train_file qrcd/qrcd_v1.1_train.jsonl \\\n",
        "  --validation_file qrcd/qrcd_v1.1_dev.jsonl \\\n",
        "  --test_file qrcd/qrcd_v1.1_my_test.jsonl \\\n",
        "  --save_total_limit 2 \\\n",
        "  --save_strategy \"epoch\" \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --load_best_model_at_end  True \\\n",
        "  --metric_for_best_model 'pRR' \\\n",
        "  --greater_is_better True \\\n",
        "  --eval_metric \"./data/qrcd_metric.py\" \n",
        "\n",
        "!git pull\n",
        "!rm -r \"MARBERT\"\n",
        "!python run_qa.py \\\n",
        "  --model_name_or_path  \"UBC-NLP/MARBERT\" \\\n",
        "  --dataset \"data/qrcd_dataset_loader.py\" \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 50 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --max_answer_length 35 \\\n",
        "  --output_dir \"MARBERT\" \\\n",
        "  --overwrite_output_dir  \\\n",
        "  --overwrite_cache \\\n",
        "  --train_file qrcd/qrcd_v1.1_train.jsonl \\\n",
        "  --validation_file qrcd/qrcd_v1.1_dev.jsonl \\\n",
        "  --test_file qrcd/qrcd_v1.1_my_test.jsonl \\\n",
        "  --save_total_limit 2 \\\n",
        "  --save_strategy \"epoch\" \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --load_best_model_at_end  True \\\n",
        "  --metric_for_best_model 'pRR' \\\n",
        "  --greater_is_better True \\\n",
        "  --eval_metric \"./data/qrcd_metric.py\" "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3HN9JYTbjJ7l"
      ],
      "name": "QRCD demo.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}