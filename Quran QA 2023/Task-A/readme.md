# Quran QA 2023 shared task A participation

Use the `TCE QQA2023-A demo notebook.ipynb` file to get an overview of:
1. installation
2. data download
3. pre-training data generation
4. training and fine-tuning of a LM
5. downloading and obtaining a dump file

ðŸŽ‰ All models can be found on my [huggingface account](https://huggingface.co/MatMulMan)

**Once the training is made you will find a dump file saved!**

something like: araelectra-base-discriminator-tafseer-pairs-fine-tuned-1e-06-5254-train.zip
This is an araelectra-base-discriminator-tafseer-pairs fine-tuned model with:
1. learning rate of 1e-06.
2. A random starting seed of 5254.
4. train.zip means training data is used

This dump file contains models prediction for the given eval or test data.

You can look at the **analysis** directory of the repo for more details.
You can group dump files into folders:
1. run **performance_analysis.py** script to process and get results for single models and ensemble models
   - **retrieval_ensemble.py** is consumed by **performance_analysis.py** to implement the ensemble logic

```
â”œâ”€â”€ analysis
          â”œâ”€â”€ dataset_basic_analysis.py (fining basic statistics)
          â”œâ”€â”€ draw_threshold_curve.py  (effect of thresholding analysis)
          â”œâ”€â”€ perfromance_analysis.py (script to analyse dump files)
          â”œâ”€â”€ retrieval_ensemble.py (script to apply ensemble on all trained models in subfolders)
â”œâ”€â”€ artifacts â”œâ”€â”€ artifacts (generated dumps, runs and submissions (download this))
â”œâ”€â”€ configs (Training configs)
â”œâ”€â”€ data (all generated and downloaed data is here)
â”œâ”€â”€ data_scripts (code for dataloading and generation for pretraining)
â”œâ”€â”€ lexical (code for lexical matching baselines)
â”œâ”€â”€ metrics (code for evaluation metrics)
â”œâ”€â”€ readme.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ biencoder (A clone of DRhard repo (STAR algorithm))
â”œâ”€â”€ cross_encoder (code for training and inferrence for cross-encoders)
â””â”€â”€ sbert (code for training and inferrence for sbert bi-encoders)
```
